{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1aNWCX5eAIwi",
    "outputId": "6023a5ed-9b73-49c4-f293-03c3d51f3506"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1 of 5\n",
      "Fold accuracy = 88.57142857142857\n",
      "Training fold 2 of 5\n",
      "Fold accuracy = 90.0\n",
      "Training fold 3 of 5\n",
      "Fold accuracy = 88.57142857142857\n",
      "Training fold 4 of 5\n",
      "Fold accuracy = 88.57142857142857\n",
      "Training fold 5 of 5\n",
      "Fold accuracy = 80.0\n",
      "Mean Accuracy: 87.143%\n"
     ]
    }
   ],
   "source": [
    "# How To Implement Learning Vector Quantization (LVQ) From Scratch With Python\n",
    "# https://machinelearningmastery.com/implement-learning-vector-quantization-scratch-python/\n",
    "\n",
    "# LVQ for the Ionosphere Dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    "\n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tn = 0\n",
    "\tfor fold in folds:\n",
    "\t\tn += 1\n",
    "\t\tprint(\"Training fold\", n, \"of\", n_folds)\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\t\tprint(\"Fold accuracy =\", accuracy)\n",
    "\treturn scores\n",
    "\n",
    "# calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "\tdistance = 0.0\n",
    "\tfor i in range(len(row1)-1):\n",
    "\t\tdistance += (row1[i] - row2[i])**2\n",
    "\treturn sqrt(distance)\n",
    "\n",
    "# Locate the best matching unit\n",
    "def get_best_matching_unit(codebooks, test_row):\n",
    "\tdistances = list()\n",
    "\tfor codebook in codebooks:\n",
    "\t\tdist = euclidean_distance(codebook, test_row)\n",
    "\t\tdistances.append((codebook, dist))\n",
    "\tdistances.sort(key=lambda tup: tup[1])\n",
    "\treturn distances[0][0]\n",
    "\n",
    "# Make a prediction with codebook vectors\n",
    "def predict(codebooks, test_row):\n",
    "\tbmu = get_best_matching_unit(codebooks, test_row)\n",
    "\treturn bmu[-1]\n",
    "\n",
    "# Create a random codebook vector\n",
    "def random_codebook(train):\n",
    "\tn_records = len(train)\n",
    "\tn_features = len(train[0])\n",
    "\tcodebook = [train[randrange(n_records)][i] for i in range(n_features)]\n",
    "\treturn codebook\n",
    "\n",
    "# Train a set of codebook vectors\n",
    "def train_codebooks(train, n_codebooks, lrate, epochs):\n",
    "\tcodebooks = [random_codebook(train) for i in range(n_codebooks)]\n",
    "\tfor epoch in range(epochs):\n",
    "\t\trate = lrate * (1.0-(epoch/float(epochs)))\n",
    "\t\tfor row in train:\n",
    "\t\t\tbmu = get_best_matching_unit(codebooks, row)\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\terror = row[i] - bmu[i]\n",
    "\t\t\t\tif bmu[-1] == row[-1]:\n",
    "\t\t\t\t\tbmu[i] += rate * error\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tbmu[i] -= rate * error\n",
    "\treturn codebooks\n",
    "\n",
    "# LVQ Algorithm\n",
    "def learning_vector_quantization(train, test, n_codebooks, lrate, epochs):\n",
    "\tcodebooks = train_codebooks(train, n_codebooks, lrate, epochs)\n",
    "\tpredictions = list()\n",
    "\tfor row in test:\n",
    "\t\toutput = predict(codebooks, row)\n",
    "\t\tpredictions.append(output)\n",
    "\treturn(predictions)\n",
    "\n",
    "# Test LVQ on Ionosphere dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'ionosphere.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# evaluate algorithm\n",
    "n_folds_original = 5\n",
    "learn_rate_original = 0.3\n",
    "n_epochs_original = 50\n",
    "n_codebooks_original = 20\n",
    "scores = evaluate_algorithm(dataset, learning_vector_quantization, n_folds_original, n_codebooks_original, learn_rate_original, n_epochs_original)\n",
    "#print('Scores: %s' % scores)\n",
    "mean_accuracy_original = sum(scores)/float(len(scores))\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1 of 6\n",
      "Fold accuracy = 42.0\n",
      "Training fold 2 of 6\n",
      "Fold accuracy = 55.00000000000001\n",
      "Training fold 3 of 6\n",
      "Fold accuracy = 46.0\n",
      "Training fold 4 of 6\n",
      "Fold accuracy = 46.0\n",
      "Training fold 5 of 6\n",
      "Fold accuracy = 38.0\n",
      "Training fold 6 of 6\n",
      "Fold accuracy = 56.99999999999999\n",
      "Mean Accuracy: 47.333%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tn = 0\n",
    "\tfor fold in folds:\n",
    "\t\tn += 1\n",
    "\t\tprint(\"Training fold\", n, \"of\", n_folds)\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\t\tprint(\"Fold accuracy =\", accuracy)\n",
    "\treturn scores\n",
    "\n",
    "# calculate the manhattan distance between two vectors\n",
    "def manhattan_distance(row1,row2):\n",
    "    distance = 0.0\n",
    "    for i in range(len(row1)-1):\n",
    "        distance += abs(row1[i] - row2[i])\n",
    "    return distance\n",
    "\n",
    "# Locate the best matching unit\n",
    "def get_best_matching_unit(codebooks, test_row):\n",
    "\tdistances = list()\n",
    "\tfor codebook in codebooks:\n",
    "\t\tdist = manhattan_distance(codebook, test_row)\n",
    "\t\tdistances.append((codebook, dist))\n",
    "\tdistances.sort(key=lambda tup: tup[1])\n",
    "\treturn distances[0][0]\n",
    "\n",
    "# Make a prediction with codebook vectors\n",
    "def predict(codebooks, test_row):\n",
    "\tbmu = get_best_matching_unit(codebooks, test_row)\n",
    "\treturn bmu[-1]\n",
    "\n",
    "# Create a random codebook vector\n",
    "def random_codebook(train):\n",
    "\tn_records = len(train)\n",
    "\tn_features = len(train[0])\n",
    "\tcodebook = [train[randrange(n_records)][i] for i in range(n_features)]\n",
    "\treturn codebook\n",
    "\n",
    "# Train a set of codebook vectors\n",
    "def train_codebooks(train, n_codebooks, lrate, epochs):\n",
    "\tcodebooks = [random_codebook(train) for i in range(n_codebooks)]\n",
    "\tfor epoch in range(epochs):\n",
    "\t\trate = lrate * (1.0-(epoch/float(epochs)))\n",
    "\t\tfor row in train:\n",
    "\t\t\tbmu = get_best_matching_unit(codebooks, row)\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\terror = row[i] - bmu[i]\n",
    "\t\t\t\tif bmu[-1] == row[-1]:\n",
    "\t\t\t\t\tbmu[i] += rate * error\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tbmu[i] -= rate * error\n",
    "\treturn codebooks\n",
    "\n",
    "# LVQ Algorithm\n",
    "def learning_vector_quantization(train, test, n_codebooks, lrate, epochs):\n",
    "\tcodebooks = train_codebooks(train, n_codebooks, lrate, epochs)\n",
    "\tpredictions = list()\n",
    "\tfor row in test:\n",
    "\t\toutput = predict(codebooks, row)\n",
    "\t\tpredictions.append(output)\n",
    "\treturn(predictions)\n",
    "\n",
    "\n",
    "\n",
    "# evaluate algorithm\n",
    "n_folds = 6\n",
    "learn_rate = 0.1\n",
    "n_epochs = 200\n",
    "n_codebooks = 30\n",
    "dataset = load_csv(\"Undergraduate Admission Test Survey in Bangladesh.csv\")\n",
    "dataset = dataset[1:]\n",
    "for i in range(len(dataset[0])-1):\n",
    "    str_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "scores = evaluate_algorithm(dataset, learning_vector_quantization, n_folds, n_codebooks, learn_rate, n_epochs)\n",
    "#print('Scores: %s' % scores)\n",
    "mean_accuracy = sum(scores)/float(len(scores))\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo original\n",
      "Accuracy original: 87.14285714285714 Hiperparámetros: n_folds: 5 learn_rate: 0.3 n_epochs: 50 n_codebooks: 20\n",
      "Modelo nuevo\n",
      "Accuracy nuevo: 47.333333333333336 Hiperparámetros: n_folds: 6 learn_rate: 0.1 n_epochs: 200 n_codebooks: 30\n"
     ]
    }
   ],
   "source": [
    "print(\"Modelo original\")\n",
    "print(\"Accuracy original:\", mean_accuracy_original, \"Hiperparámetros:\", \"n_folds:\", n_folds_original, \"learn_rate:\", learn_rate_original, \"n_epochs:\", n_epochs_original, \"n_codebooks:\", n_codebooks_original)\n",
    "print(\"Modelo nuevo\")\n",
    "print(\"Accuracy nuevo:\", mean_accuracy, \"Hiperparámetros:\", \"n_folds:\", n_folds, \"learn_rate:\", learn_rate, \"n_epochs:\", n_epochs, \"n_codebooks:\", n_codebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusión:\n",
    "\n",
    "El dataset usado es un dataset para predecir en que tipo de universidad quedo un estudiante de reino unido, si a una escuela privada o a una escuela publica. Lo cual hace el problema aun más dificil, pues no solo depende de factores del pasado o actividades, a veces esto puede ser cuestión de suerte.\n",
    "\n",
    "La primera modificación fue cambiar la función de distancia de Euclidean a **Manhattan**. Este ajuste fue rápido de implementar. Sin embargo es importante notar que esta métrica es una de las más influyentes en un modelo como este, pues es la manera en la que evalua sus soluciones y la manera en la que entrena.\n",
    "\n",
    "En cuanto a la arquitectura, incrementé el número de **codebooks** a 30. La modificación de este parámetro puede mejorar la precisión del modelo, ya que un mayor número de codebooks permite una mejor representación del espacio de características, sin embargo reduce la velocidad de computo del modelo, asi como puede llevar a sobreentrenamiento.\n",
    "\n",
    "De igual manera, ajusté la tasa de aprendizaje y el número de épocas. Aumentar las épocas a 200 y reducir la tasa de aprendizaje a 0.1 hizo que el proceso fuera más lento a la hora de entrenar, pero se permitio que el modelo entrenara un mayor numero de veces.\n",
    "\n",
    "Esta actividad me ayudó a entender el algoritmo. Asi como aprendí como el cambiar los hiperparametros una vez que entiendes el algoritmo es importante para obtener mejores resultados. Asi como es importante balancear el rendimiento del modelo, asi como su coste computacional"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LVQ.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "kirass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
